[
["index.html", "EDS Vignettes Prerequisites", " EDS Vignettes Andry Rajaoberison 2020-06-24 Prerequisites The following tutorials are written in R. To install R on your computer, please visit this link: https://www.r-project.org/ You should also install RStudio: https://rstudio.com/products/rstudio/download/ We will use the packages in the tidyverse library: install.packages(&quot;tidyverse&quot;) We will use a sample datasets from the Yale Program on Climate Change Communication. The advantage of these data is that they were collected and processed by our colleagues. So we have a good documentation on the sampling methods and the data processing workflow applied to eah of the datasets. "],
["reading.html", "Section 1 Reading 1.1 CSV 1.2 Excel 1.3 Google Spreadsheets 1.4 SPSS", " Section 1 Reading In this section, we learn how to read data from different file format. 1.1 CSV Reading csv with column headers and separated by ,. These parameters are also the default values for the read.csv function. data &lt;- read.csv(file = &#39;/path/to/csv&#39;, header = TRUE, sep = &#39;,&#39;) # Example data &lt;- read.csv(file = &#39;data/hurricane.csv&#39;, header = TRUE) 1.2 Excel The main advantage of Excel files is that they can store multiple tables. But reading these tables at once is different from a CSV. For this example, we’re going to use the readxl package from the tidyverse collection. Please visit this [website] (https://www.tidyverse.org/) to learn more about tidyverse. To read an excel file, you can use the read_excel function and specify at least the path/to/the/file and sheet you want to open. If you don’t specify the sheet, read_excel will automatically open the first table in the spreadsheet. In the ‘eds.excel.sample.xlsx’ file, there are 2 tables: heatwave and hurricane. Here’s how we load both tables into R: library(readxl) heatwave &lt;- read_excel(path=&#39;data/excel-sample.xlsx&#39;, sheet = &#39;heatwave&#39;) hurricane &lt;- read_excel(path=&#39;data/excel-sample.xlsx&#39;, sheet = &#39;hurricane&#39;) Once the tables are stored into individual R variable, you can perform exploration and analysis with them. 1.3 Google Spreadsheets If the data is stored in a Google spreadsheet, we can read it using the googledrive and googlesheet4 packages. We use the googledrive package to log into our Google Drive account and googlesheets4 to read the speadsheets in our drive. In the example below, I used a spreadsheet named eds.sample.googlesheets which contains the same tables in the previous Excel example (heatwave and hurricane). You can clone the spreadsheet via this [link] (https://drive.google.com/open?id=1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc if you’d like to repeat the steps below using your Google account. Then authenticate to your drive using drive_auth(). When prompted: log in, authorized googledrive, and use the authorization code if provided. You only need to run drive_auth() once. library(googledrive) # To authenticate and authorize googledrive package drive_auth() The following scripts show how to explore a Google Drive folder. Though this is not recommended as you might encounter performance issues. # NOT recommended # Then, to view the list of files in a folder drive_ls(&quot;EDS&quot;) # where &quot;EDS&quot; is the folder name # To also get the files within the subfolders drive_ls(&quot;EDS&quot;, recursive = TRUE) # To view the list of spreadsheets within a folder drive_ls(&quot;EDS&quot;, type=&quot;spreadsheet&quot;) Also, because of Google authentification system, you may run into an error like below when running the previous code (using drive_ls()). Which is why it’s not recommended. #&gt; Error in add_id_path(nodes, root_id = root_id, leaf = leaf) : !anyDuplicated(nodes$id) is not TRUE To avoid this, you can use the folder url instead of the folder name. The folder url can be obtained by right-clicking on the folder and click Get shareable link. Then run the following code: # If using folder name doesn&#39;t work folder_url = &#39;https://drive.google.com/open?id=1e0uJ9dwFcL34JA61F0tGSoaiMZ_xio_4&#39; drive_ls(folder_url, type=&quot;spreadsheet&quot;) Then you can load the spreadsheet by using its id eds.sample.spreadsheet &lt;- drive_get(id = &#39;1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc&#39;) It also possible to read the spreadsheet right way by using its link/path (without using drive_ls()). I recommend using this to read any Google Drive files. eds.sample.spreadsheet &lt;- drive_get(path = &#39;https://drive.google.com/open?id=1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc&#39;) Once the spreadsheet is loaded, we run a similar code used for the Excel files to read tables within the spreadsheet. But for Google Sheets, function is called read_sheet library(googlesheets4) # Authorizing the googlesheets4 package sheets_auth(token=drive_token()) # Reading the tables heatwave &lt;- read_sheet(eds.sample.spreadsheet, sheet = &#39;heatwave&#39;) hurricane &lt;- read_sheet(eds.sample.spreadsheet, sheet = &#39;hurricane&#39;) 1.4 SPSS library(haven) data &lt;- read_sav(&quot;data/spss-sample.sav&quot;) By default, the read_sav() will read the factor levels of non-numeric and non-character variables. If, instead, we want the labels, we can run the following code: library(magrittr) library(dplyr) # Applying haven::as_factor() to labelled columns Here, we already know that # variables Zone, Q4 and Q50 are not factor variables. data %&gt;% mutate_at(vars(-Zone, -Q4, -Q50), as_factor) ## # A tibble: 1,130 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 Moderately Pr… No 1928 Male $70,000-$99… ## 2 A 1 4 0 Moderately Pr… No 1962 Male &lt;NA&gt; ## 3 A 3 4 0 Moderately Pr… No 1931 Fema… Over $200,0… ## 4 A 3 6 1 Fully Prepared No 1950 Male $100,000-$1… ## 5 A 2 Not Worried … 0 Very Prepared No 1948 Male $100,000-$1… ## 6 A 5 4 0 Very Prepared No 1938 Fema… &lt;NA&gt; ## 7 A 3 6 1 Moderately Pr… No 1977 Fema… &lt;NA&gt; ## 8 A 5 4 0 Moderately Pr… No 1964 Fema… &lt;NA&gt; ## 9 A 1 3 0 Moderately Pr… No 1976 Male $40,000-$69… ## 10 A 2 6 0 Very Prepared No 1964 Fema… Over $200,0… ## # … with 1,120 more rows Because variables can be labelled in SPSS, we can use them as well to find out what each column represents. # To get the labels of the variables / columns as.vector(unlist(lapply(data, function(x) attributes(x)$label))) ## [1] &quot;Q4. Since the beginning of 2009, how many hurricanes and tropical storms, if any, hit your city or town on or near the Connecticut coast while you were at home; that is, not out of town?&quot; ## [2] &quot;Q5. Generally speaking, when a hurricane or tropical storm is approaching your city or town, how worried do you feel? Please answer using the following scale ranging from 1 (not at all worried) to 7 (extremely worried).&quot; ## [3] &quot;Q6. Since the beginning of 2009, how many times, if ever, did you leave your home for someplace safer to avoid a hurricane or tropical storm; that is, how many times did you evacuate? Please enter the number in the boxes below.&quot; ## [4] &quot;Q7. Generally speaking, how prepared were you for the storm(s) you experienced?&quot; ## [5] &quot;Q10. Before Superstorm Sandy hit your area, did you leave your home to go someplace safer; that is, did you evacuate?&quot; ## [6] &quot;Q50. In what year were you born?&quot; ## [7] &quot;Q51. Are you...?&quot; ## [8] &quot;Q59. Last year (in 2013), what was your total HOUSEHOLD income from all sources?&quot; To learn more about the haven package and how the variables are stored, please visit: https://haven.tidyverse.org/ "],
["extraction.html", "Section 2 Extraction 2.1 Web Scraping", " Section 2 Extraction 2.1 Web Scraping Web scraping is the process of fetching and extracting information / data from a webpage. It is very useful if you want to create a dynamic database that updates based on the content of a specific website. To scrap a webpage, we first need to know how to get to the webpage, a url that you can use to directly access the content. For example, to obtain the Google search results for “data science”, you can simply copy and paste this url to your browser: https://www.google.com/search?q=data+science, without having to type “data science” on a Google search web page. Some websites like Twitter or Facebook will require to you to use an API and authenticate in order to access some of their data. For this example, we’re going to use The Weather Channel website which do not require autentification. We’ll to extract the 10-day forecast for a specific location and store the data in a dataframe. After inspecting the website and it’s url, I have noticed that you can view the weather data by zip code using this url pattern: https://weather.com/weather/ + forecast type + /l/ + zip_code + :4:US For example, if we want to view the 10-day forecast for New Haven, we can go to: https://weather.com/weather/tenday/l/06511:4:US. And for today’s forecast: https://weather.com/weather/today/l/06511:4:US Once we have the webpage url, we can read it into R and extract the data using rvest from the tidyverse collection. The New Haven 10-day forecast webpage looks like this: weatherpage Basically, what we want is the table that has the weather information. In order to extract the values that we want, we have to know where in the source code they are located. For example, in the “DAY” column, we want to extract the exact date instead of the days of the week. And we can do that by: inspecting the tag or class of exact date from the website. Move the cursor to the exact date, right-click, then choose Inspect then, a window will open, which will point directly to the location of the exact date in the source code. Take notes of the css (tag or class name), and use it to get the exact date value using the html_nodes() function. weatherpage Here is how we extract the dates: library(rvest) ## Loading required package: xml2 # Get the webpage url url = &#39;https://weather.com/weather/tenday/l/06511:4:US&#39; # Load the webpage using the url webpage &lt;- read_html(url) # Getting the exact date # Filtering the relevant css / location date_locations &lt;- html_nodes(webpage, &quot;span.day-detail.clearfix&quot;) # Extracting the exact value raw_date &lt;- html_text(date_locations) raw_date ## character(0) # Because the value are formatted like &quot;NOV 21&quot; we have to convert to a date format exact_date &lt;- as.Date(raw_date, format=&quot;%b %d&quot;) # b = month, d = day exact_date ## Date of length 0 And here is the full code that extract the complete table: library(rvest) # Get the webpage url url = &#39;https://weather.com/weather/tenday/l/06511:4:US&#39; # Load the webpage using the url webpage &lt;- read_html(url) # Getting the exact date # Filtering the relevant css / location date_locations &lt;- html_nodes(webpage, &quot;span.day-detail.clearfix&quot;) # Extracting the exact value raw_date &lt;- html_text(date_locations) # Because the value are formatted like &quot;Nov 21&quot; we have to convert to a date format exact_date &lt;- as.Date(raw_date, format=&quot;%b %d&quot;) # b = month, d = day # Getting the weather description desc_loc &lt;- html_nodes(webpage, &quot;td.description&quot;) desc &lt;- html_text(desc_loc) # Getting the temperature temp_loc &lt;- html_nodes(webpage, &quot;td.temp&quot;) temp &lt;- html_text(temp_loc) # High and Low temperature values high_temp &lt;- rep(NA, length(temp)) low_temp &lt;- rep(NA, length(temp)) for (i in 1:length(temp)){ all &lt;- unlist(strsplit(temp[i], &quot;°&quot;)) if (length(all) &gt; 1){ high_temp[i] &lt;- all[1] low_temp[i] &lt;- all[2] } else { low_temp[i] &lt;- 38 } } # Getting the precipitation precip_loc &lt;- html_nodes(webpage, &quot;td.precip&quot;) precip &lt;- as.numeric(sub(&quot;%&quot;, &quot;&quot;, html_text(precip_loc))) / 100 # Getting the wind wind_loc &lt;- html_nodes(webpage, &quot;td.wind&quot;) wind &lt;- html_text(wind_loc) # Wind direction and speed wind_dir &lt;- rep(NA, length(wind)) wind_speed &lt;- rep(NA, length(wind)) for (i in 1:length(wind)){ all &lt;- unlist(strsplit(wind[i], &quot; &quot;)) wind_dir[i] &lt;- all[1] wind_speed[i] &lt;- all[2] } # Getting the humidity humidity_loc &lt;- html_nodes(webpage, &quot;td.humidity&quot;) humidity &lt;- as.numeric(sub(&quot;%&quot;, &quot;&quot;, html_text(humidity_loc))) / 100 # Save the data in tibble library(tibble) new_haven_forecast &lt;- tibble(&#39;day&#39; = exact_date, &#39;description&#39; = desc, &#39;high_temp&#39; = high_temp, &#39;low_temp&#39; = low_temp, &#39;precip&#39; = precip, &#39;wind_dir&#39; = wind_dir, &#39;wind_speed&#39; = wind_speed, &#39;himidity&#39; = humidity) new_haven_forecast ## # A tibble: 0 x 8 ## # … with 8 variables: day &lt;date&gt;, description &lt;chr&gt;, high_temp &lt;lgl&gt;, ## # low_temp &lt;dbl&gt;, precip &lt;dbl&gt;, wind_dir &lt;chr&gt;, wind_speed &lt;chr&gt;, ## # himidity &lt;dbl&gt; "],
["structuring.html", "Section 3 Structuring 3.1 Inspecting the data 3.2 Data types 3.3 Subsetting and Filtering 3.4 Changing cell values 3.5 Pivoting the dataset", " Section 3 Structuring Data structuring is the process of correcting or removing inaccurate records of a “raw data” so that, after the treatment, the transformed data will be easy to analyze and/or consistent with an existing dataset. More explicitly, the variable names, types, and values will be consistent and uniform. The focus here is on the ‘appearance’ of the data. 3.1 Inspecting the data In order to structure a dataset, first, we need to be able to detect the anomalies within the data. Types of anomalies include the values that are stored in the wrong format (ex: a number stored as a string), the values that fall outside of the expected range (ex: outliers), values with inconsistent patterns (ex: dates stored as mm/dd/year vs dd/mm/year), trailing spaces in strings (ex: “data” vs “data”), etc. One method of detecting these anomalies is the summary statistics of the variables, which can be obtained by using summary(). Here is an example using the hurricane data: # Structure of the data str(data[,1:3]) ## tibble [1,130 × 3] (S3: tbl_df/tbl/data.frame) ## $ Zone: chr [1:1130] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A9&quot; ## ..- attr(*, &quot;display_width&quot;)= int 1 ## $ Q4 : num [1:1130] 2 1 3 3 2 5 3 5 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Q4. Since the beginning of 2009, how many hurricanes and tropical storms, if any, hit your city or town on or n&quot;| __truncated__ ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 2 ## $ Q5 : dbl+lbl [1:1130] 3, 4, 4, 6, 1, 4, 6, 4, 3, 6, 7, 5, 7, ... ## ..@ label : chr &quot;Q5. Generally speaking, when a hurricane or tropical storm is approaching your city or town, how worried do you&quot;| __truncated__ ## ..@ format.spss: chr &quot;F1.0&quot; ## ..@ labels : Named num [1:2] 1 7 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not Worried At All&quot; &quot;Extremely Worried&quot; # Summary for a numerical variables summary(data$Q4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 2.000 2.000 2.537 3.000 20.000 134 # Summary for a categorical variable summary(as_factor(data$Q7)) ## Fully Prepared Very Prepared Moderately Prepared A Little Prepared ## 93 326 438 137 ## Not at all Prepared NA&#39;s ## 22 114 Other ways of exploring the data include: # First 10 rows head(data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; ## 1 A 2 3 0 3 [Moderate… 2 [No] 1928 1 [Mal… 4 [$70,00… ## 2 A 1 4 0 3 [Moderate… 2 [No] 1962 1 [Mal… NA ## 3 A 3 4 0 3 [Moderate… 2 [No] 1931 2 [Fem… 6 [Over $… ## 4 A 3 6 1 1 [Fully Pr… 2 [No] 1950 1 [Mal… 5 [$100,0… ## 5 A 2 1 [Not Worr… 0 2 [Very Pre… 2 [No] 1948 1 [Mal… 5 [$100,0… ## 6 A 5 4 0 2 [Very Pre… 2 [No] 1938 2 [Fem… NA ## 7 A 3 6 1 3 [Moderate… 2 [No] 1977 2 [Fem… NA ## 8 A 5 4 0 3 [Moderate… 2 [No] 1964 2 [Fem… NA ## 9 A 1 3 0 3 [Moderate… 2 [No] 1976 1 [Mal… 3 [$40,00… ## 10 A 2 6 0 2 [Very Pre… 2 [No] 1964 2 [Fem… 6 [Over $… # Last 10 rows tail(data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+lbl&gt; ## 1 B 1 2 0 4 [A Little P… 2 [No] 1980 1 [Male] 3 [$40,000-… ## 2 B 2 2 0 3 [Moderately… 2 [No] 1977 2 [Fema… 4 [$70,000-… ## 3 B 4 4 1 2 [Very Prepa… 1 [Yes] 1962 2 [Fema… 2 [$15,000-… ## 4 B 2 5 0 1 [Fully Prep… 2 [No] 1946 1 [Male] 5 [$100,000… ## 5 B NA 4 NA 1 [Fully Prep… 1 [Yes] 1957 2 [Fema… 1 [Less tha… ## 6 B 1 4 1 4 [A Little P… 1 [Yes] 1987 2 [Fema… 6 [Over $20… ## 7 B 2 5 0 3 [Moderately… 2 [No] 1953 1 [Male] 4 [$70,000-… ## 8 B NA 4 4 2 [Very Prepa… 2 [No] 1973 2 [Fema… 1 [Less tha… ## 9 B 2 5 0 3 [Moderately… 2 [No] 1980 1 [Male] 5 [$100,000… ## 10 B 2 2 0 4 [A Little P… 2 [No] NA 2 [Fema… 3 [$40,000-… # Total number of rows nrow(data) ## [1] 1130 # Total number of columns ncol(data) ## [1] 9 # Column names names(data) # also colnames(data) ## [1] &quot;Zone&quot; &quot;Q4&quot; &quot;Q5&quot; &quot;Q6&quot; &quot;Q7&quot; &quot;Q10&quot; &quot;Q50&quot; &quot;Q51&quot; &quot;Q59&quot; We can also plot the data to visualize the distribution of variables # Plotting the first 5 columns plot(data[,1:5]) While these plots could help in understanding the dataset, they could be misleading if the variables are not set to their correct data type. 3.2 Data types One type of anomaly that we may also encounter is the coercion of irrelevant data types to a variables. This is very common for numerically coded variables or ones that has levels. For example, if we read in the same SPSS data from the Reading data section, we get the coded values instead of the labels. ## # A tibble: 6 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; ## 1 A 2 3 0 3 [Moderate… 2 [No] 1928 1 [Mal… 4 [$70,000… ## 2 A 1 4 0 3 [Moderate… 2 [No] 1962 1 [Mal… NA ## 3 A 3 4 0 3 [Moderate… 2 [No] 1931 2 [Fem… 6 [Over $2… ## 4 A 3 6 1 1 [Fully Pr… 2 [No] 1950 1 [Mal… 5 [$100,00… ## 5 A 2 1 [Not Worr… 0 2 [Very Pre… 2 [No] 1948 1 [Mal… 5 [$100,00… ## 6 A 5 4 0 2 [Very Pre… 2 [No] 1938 2 [Fem… NA So if we run summary(data) right away then we’ll get this unintended result: ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Zone Q4 Q5 Q6 ## Min. : NA Min. : 0.000 Min. :1.000 Min. :0.0000 ## 1st Qu.: NA 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:0.0000 ## Median : NA Median : 2.000 Median :2.000 Median :0.0000 ## Mean :NaN Mean : 2.537 Mean :1.617 Mean :0.4191 ## 3rd Qu.: NA 3rd Qu.: 3.000 3rd Qu.:2.000 3rd Qu.:1.0000 ## Max. : NA Max. :20.000 Max. :2.000 Max. :4.0000 ## NA&#39;s :1130 NA&#39;s :134 NA&#39;s :968 NA&#39;s :116 ## Q7 Q10 Q50 Q51 Q59 ## Min. :1.000 Min. :1.000 Min. : 19 Min. :1.00 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:1944 1st Qu.:1.00 1st Qu.:3.000 ## Median :3.000 Median :2.000 Median :1955 Median :2.00 Median :4.000 ## Mean :2.674 Mean :1.796 Mean :1944 Mean :1.55 Mean :3.715 ## 3rd Qu.:3.000 3rd Qu.:2.000 3rd Qu.:1966 3rd Qu.:2.00 3rd Qu.:5.000 ## Max. :5.000 Max. :2.000 Max. :1992 Max. :2.00 Max. :6.000 ## NA&#39;s :114 NA&#39;s :123 NA&#39;s :47 NA&#39;s :38 NA&#39;s :112 Q4 and Q50 are the only variables that are supposed to be numeric. But here everything is treated as numeric which is incorrect. Also, it is best if we read Zone as factor as well so that we find out the possible values. We can easily convert data types into factor using dplyr::mutate_at() and applying as.factor function to the variables. # Converting data types updated_data &lt;- data %&gt;% mutate_at(vars(-Q4, -Q6, -Q50), as_factor) And now we can get the full summary statistics that we want: ## Zone Q4 Q5 Q6 ## A:684 Min. : 0.000 5 :244 Min. :0.0000 ## B:446 1st Qu.: 2.000 4 :211 1st Qu.:0.0000 ## Median : 2.000 3 :169 Median :0.0000 ## Mean : 2.537 6 :129 Mean :0.4191 ## 3rd Qu.: 3.000 2 :104 3rd Qu.:1.0000 ## Max. :20.000 (Other):162 Max. :4.0000 ## NA&#39;s :134 NA&#39;s :111 NA&#39;s :116 ## Q7 Q10 Q50 Q51 ## Fully Prepared : 93 Yes :205 Min. : 19 Male :491 ## Very Prepared :326 No :802 1st Qu.:1944 Female:601 ## Moderately Prepared:438 NA&#39;s:123 Median :1955 NA&#39;s : 38 ## A Little Prepared :137 Mean :1944 ## Not at all Prepared: 22 3rd Qu.:1966 ## NA&#39;s :114 Max. :1992 ## NA&#39;s :47 ## Q59 ## Less than $15,000: 81 ## $15,000-$39,999 :169 ## $40,000-$69,999 :215 ## $70,000-$99,999 :190 ## $100,000-$199,999:220 ## Over $200,000 :143 ## NA&#39;s :112 As we can see from the summary, there might be some anomalies with the variables: Zone: as most of the respondents are from Zone A. But this is basically related to the survey method which would later require that some weighting of the variables would be applied. Q4: Number of storms experienced: where the mean value is 2.5 but some response have the value of 20. Q50: Birth year: where some respondent answered 19 which is incorrect. Also this column is probably better if it’s in age instead of birth year. We can also notice some missing values. 3.3 Subsetting and Filtering We can remove incorrect or missing row values by using dplyr::filter: # Removing rows where birth year is irrelevant # Here we decided that all birth year must be greater 1900 updated_data &lt;- data %&gt;% filter(Q50 &gt; 1900) # Now if we re-run its summary summary(updated_data$Q50) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1908 1945 1955 1956 1966 1992 # Removing rows with birth year greater than 1900 and missing responses for Q4 updated_data &lt;- data %&gt;% filter(Q50 &gt; 1900, !is.na(Q4)) summary(updated_data$Q50) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1908 1944 1954 1955 1965 1990 summary(updated_data$Q4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 2.000 2.522 3.000 20.000 We can also select only the variables that we are interested in using dplyr::select: # Creating a new dataframe with only zone, gender, and income column updated_data &lt;- data %&gt;% select(Zone, Q59, Q51) head(updated_data, 10) ## # A tibble: 10 x 3 ## Zone Q59 Q51 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A $70,000-$99,999 Male ## 2 A &lt;NA&gt; Male ## 3 A Over $200,000 Female ## 4 A $100,000-$199,999 Male ## 5 A $100,000-$199,999 Male ## 6 A &lt;NA&gt; Female ## 7 A &lt;NA&gt; Female ## 8 A &lt;NA&gt; Female ## 9 A $40,000-$69,999 Male ## 10 A Over $200,000 Female plot(table(updated_data), las=1) It is also possible to split the dataset into multiple dataframe by number of rows using split(). # To split the dataset into multiple dataframe of 10 rows each max_number_of_rows_per_dataframe &lt;- 10 total_number_rows_in_the_current_dataset &lt;- nrow(data) sets_of_10rows_dataframes &lt;- split(data, rep(1:ceiling(total_number_rows_in_the_current_dataset/max_number_of_rows_per_dataframe), each=max_number_of_rows_per_dataframe, length.out=total_number_rows_in_the_current_dataset) ) # Here are the first 2 dataframes sets_of_10rows_dataframes[[1]] # or sets_of_10rows_dataframes$`1` ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 Moderately Pr… No 1928 Male $70,000-$99… ## 2 A 1 4 0 Moderately Pr… No 1962 Male &lt;NA&gt; ## 3 A 3 4 0 Moderately Pr… No 1931 Fema… Over $200,0… ## 4 A 3 6 1 Fully Prepared No 1950 Male $100,000-$1… ## 5 A 2 Not Worried … 0 Very Prepared No 1948 Male $100,000-$1… ## 6 A 5 4 0 Very Prepared No 1938 Fema… &lt;NA&gt; ## 7 A 3 6 1 Moderately Pr… No 1977 Fema… &lt;NA&gt; ## 8 A 5 4 0 Moderately Pr… No 1964 Fema… &lt;NA&gt; ## 9 A 1 3 0 Moderately Pr… No 1976 Male $40,000-$69… ## 10 A 2 6 0 Very Prepared No 1964 Fema… Over $200,0… sets_of_10rows_dataframes[[2]] ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 Extremely Wo… 2 Fully Prepared Yes 1937 Fema… &lt;NA&gt; ## 2 A 3 5 0 Very Prepared No 1943 Male $70,000-$99… ## 3 A 2 Extremely Wo… 0 Very Prepared No 1954 Fema… $100,000-$1… ## 4 A 2 5 0 Very Prepared No 1959 Fema… $100,000-$1… ## 5 A 4 Not Worried … NA Very Prepared No 1936 Fema… Over $200,0… ## 6 A 1 3 1 Moderately Pr… Yes 1963 Male Over $200,0… ## 7 A 2 3 1 Very Prepared Yes 1950 Fema… $100,000-$1… ## 8 A 4 6 0 Moderately Pr… No NA &lt;NA&gt; &lt;NA&gt; ## 9 A 0 4 0 Very Prepared No 1941 Male $100,000-$1… ## 10 A NA &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; 1952 Fema… $100,000-$1… 3.4 Changing cell values As we mentionned earlier, it is best if Q50 is stored as an age variable instead of the default birth year. Q50 is a numeric variable and we can simply change it by using dplyr::mutate() # Replacing Q50 values to their age in 2020 updated_data &lt;- data %&gt;% mutate(Q50 = 2020 - Q50) head(updated_data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 Moderately Pr… No 92 Male $70,000-$99… ## 2 A 1 4 0 Moderately Pr… No 58 Male &lt;NA&gt; ## 3 A 3 4 0 Moderately Pr… No 89 Fema… Over $200,0… ## 4 A 3 6 1 Fully Prepared No 70 Male $100,000-$1… ## 5 A 2 Not Worried … 0 Very Prepared No 72 Male $100,000-$1… ## 6 A 5 4 0 Very Prepared No 82 Fema… &lt;NA&gt; ## 7 A 3 6 1 Moderately Pr… No 43 Fema… &lt;NA&gt; ## 8 A 5 4 0 Moderately Pr… No 56 Fema… &lt;NA&gt; ## 9 A 1 3 0 Moderately Pr… No 44 Male $40,000-$69… ## 10 A 2 6 0 Very Prepared No 56 Fema… Over $200,0… # It is also possible to leave Q50 untouched and store the results into a new column updated_data &lt;- data %&gt;% mutate(age = 2020 - Q50) head(updated_data, 10) ## # A tibble: 10 x 10 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 Moderately … No 1928 Male $70,000-$… 92 ## 2 A 1 4 0 Moderately … No 1962 Male &lt;NA&gt; 58 ## 3 A 3 4 0 Moderately … No 1931 Fema… Over $200… 89 ## 4 A 3 6 1 Fully Prepa… No 1950 Male $100,000-… 70 ## 5 A 2 Not Worrie… 0 Very Prepar… No 1948 Male $100,000-… 72 ## 6 A 5 4 0 Very Prepar… No 1938 Fema… &lt;NA&gt; 82 ## 7 A 3 6 1 Moderately … No 1977 Fema… &lt;NA&gt; 43 ## 8 A 5 4 0 Moderately … No 1964 Fema… &lt;NA&gt; 56 ## 9 A 1 3 0 Moderately … No 1976 Male $40,000-$… 44 ## 10 A 2 6 0 Very Prepar… No 1964 Fema… Over $200… 56 summary(updated_data$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 28.00 54.00 65.00 64.14 75.00 112.00 For a categorical variable, we use a different function dplyr::recode_factor() or dplyr::recode(). We will apply this to Q5 as we have noticed in the previous section that not all of its values were labelled from SPSS. Here is its summary: ## Not Worried At All 2 3 4 ## 58 101 164 197 ## 5 6 Extremely Worried NA&#39;s ## 232 123 97 104 Looking back at the questionnare, here is how it was phrased: Because the survey itself doesn’t have labels, the recoding will be up to the user. Here we chose to remove replace the extreme values with 1 and 7. As mentionned in the documentation: dplyr::recode() will preserve the existing order of levels while changing the values, and dplyr::recode_factor() will change the order of levels to match the order of replacements. # Recoding Q5 recoded.with.recode &lt;- recode(data$Q5, `Not Worried At All`=&quot;1&quot;, `Extremely Worried`=&quot;7&quot;) summary(recoded.with.recode) ## 1 2 3 4 5 6 7 NA&#39;s ## 58 101 164 197 232 123 97 104 recoded.with.recode_factor &lt;- recode_factor(data$Q5, `Not Worried At All`=&quot;1&quot;, `Extremely Worried`=&quot;7&quot;) summary(recoded.with.recode_factor) ## 1 7 2 3 4 5 6 NA&#39;s ## 58 97 101 164 197 232 123 104 We can also change cell values without external libraries like dplyr by running the following code: # Add column age where the values are 2020 - Q50 data$age &lt;- 2020 - data$Q50 # Replace Q5 with value &quot;Not Worried At All&quot; to &quot;1&quot; data$Q5[data$Q5 == &quot;Not Worried At All&quot;] &lt;- 1 ## Warning in `[&lt;-.factor`(`*tmp*`, data$Q5 == &quot;Not Worried At All&quot;, value = ## structure(c(3L, : invalid factor level, NA generated 3.5 Pivoting the dataset In some cases, we may want to split a column based on values, or merge multiple columns into fewer columns. These process can be done using tidyr package. For example, to convert the dataframe into long-format with only Zone, question, and value as columns: library(tidyr) # We have to pivot by variable type # Pivot longer for factor variables pivoted.longer &lt;- data %&gt;% select_if(is.factor) %&gt;% pivot_longer(-Zone, names_to = &quot;question&quot;, values_to = &quot;value&quot;) pivoted.longer ## # A tibble: 5,380 x 3 ## Zone question value ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; ## 1 A Q5 3 ## 2 A Q7 Moderately Prepared ## 3 A Q10 No ## 4 A Q51 Male ## 5 A Q59 $70,000-$99,999 ## 6 A Q5 4 ## 7 A Q7 Moderately Prepared ## 8 A Q10 No ## 9 A Q51 Male ## 10 A Q59 &lt;NA&gt; ## # … with 5,370 more rows # Then we can reshape it back to the original pivoted.wider &lt;- pivoted.longer %&gt;% group_by(question) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = question, values_from = value) %&gt;% select(-row) pivoted.wider ## # A tibble: 1,076 x 6 ## Zone Q5 Q7 Q10 Q51 Q59 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 3 Moderately Prepared No Male $70,000-$99,999 ## 2 A 4 Moderately Prepared No Male &lt;NA&gt; ## 3 A 4 Moderately Prepared No Female Over $200,000 ## 4 A 6 Fully Prepared No Male $100,000-$199,999 ## 5 A &lt;NA&gt; Very Prepared No Male $100,000-$199,999 ## 6 A 4 Very Prepared No Female &lt;NA&gt; ## 7 A 6 Moderately Prepared No Female &lt;NA&gt; ## 8 A 4 Moderately Prepared No Female &lt;NA&gt; ## 9 A 3 Moderately Prepared No Male $40,000-$69,999 ## 10 A 6 Very Prepared No Female Over $200,000 ## # … with 1,066 more rows tidyr::spread() and tidyr::gather() are the outdated equivalent of tidyr::pivot_wider() and tidyr::pivot_longer(). To merge or split columns, we can use tidyr::unite() or tidyr::separate(). For example, to merge Q7 and Q10: # Creating a new column with responses from both Q7 and Q10 merged &lt;- data %&gt;% unite(&quot;Q7_Q10&quot;, Q7:Q10, sep = &quot;__&quot;, remove = TRUE, na.rm = FALSE) merged ## # A tibble: 1,076 x 9 ## Zone Q4 Q5 Q6 Q7_Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 Moderately Prepared… 1928 Male $70,000-$99,9… 92 ## 2 A 1 4 0 Moderately Prepared… 1962 Male &lt;NA&gt; 58 ## 3 A 3 4 0 Moderately Prepared… 1931 Fema… Over $200,000 89 ## 4 A 3 6 1 Fully Prepared__No 1950 Male $100,000-$199… 70 ## 5 A 2 &lt;NA&gt; 0 Very Prepared__No 1948 Male $100,000-$199… 72 ## 6 A 5 4 0 Very Prepared__No 1938 Fema… &lt;NA&gt; 82 ## 7 A 3 6 1 Moderately Prepared… 1977 Fema… &lt;NA&gt; 43 ## 8 A 5 4 0 Moderately Prepared… 1964 Fema… &lt;NA&gt; 56 ## 9 A 1 3 0 Moderately Prepared… 1976 Male $40,000-$69,9… 44 ## 10 A 2 6 0 Very Prepared__No 1964 Fema… Over $200,000 56 ## # … with 1,066 more rows # To split it back merged %&gt;% separate(Q7_Q10, c(&quot;Q7&quot;, &quot;Q10&quot;), sep = &quot;__&quot;, remove = TRUE) ## # A tibble: 1,076 x 10 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 Moderately Pre… No 1928 Male $70,000-$99,… 92 ## 2 A 1 4 0 Moderately Pre… No 1962 Male &lt;NA&gt; 58 ## 3 A 3 4 0 Moderately Pre… No 1931 Fema… Over $200,000 89 ## 4 A 3 6 1 Fully Prepared No 1950 Male $100,000-$19… 70 ## 5 A 2 &lt;NA&gt; 0 Very Prepared No 1948 Male $100,000-$19… 72 ## 6 A 5 4 0 Very Prepared No 1938 Fema… &lt;NA&gt; 82 ## 7 A 3 6 1 Moderately Pre… No 1977 Fema… &lt;NA&gt; 43 ## 8 A 5 4 0 Moderately Pre… No 1964 Fema… &lt;NA&gt; 56 ## 9 A 1 3 0 Moderately Pre… No 1976 Male $40,000-$69,… 44 ## 10 A 2 6 0 Very Prepared No 1964 Fema… Over $200,000 56 ## # … with 1,066 more rows "],
["cleaning.html", "Section 4 Cleaning 4.1 Fixing skewed distribution 4.2 Treating outliers 4.3 Fixing missing values", " Section 4 Cleaning Data cleaning is the process of getting the data ready for statistical analysis. In contrast to “Structuring the data” the target anomalies in this case are the variable values such as missing values, outliers, distribution, etc. 4.1 Fixing skewed distribution A data is skewed when it’s distribution is not symetrical but rather distored on the left or right. Sometimes, to facilitate statistical analysis, we need to transform that skewed data so that it becomes normally distributed instead. # Example of skewed data {x &lt;- data$nstorm h &lt;- hist(x, breaks=20, col=&quot;grey&quot;) xfit &lt;- seq(min(x), max(x), length=40) yfit &lt;- dnorm(xfit, mean=mean(x), sd=sd(x)) yfit &lt;- yfit*diff(h$mids[1:2])*length(x) lines(xfit, yfit, col=&quot;red&quot;, lwd=2)} # Log-transformation {log.x &lt;- log(data$nstorm) h &lt;- hist(log.x, breaks = 10, col = &quot;grey&quot;)} While the previous log-transformation seem to have worked, it is not entirely correct because the variable nstorm has 0 values. In this case, one of the commonly used method is square root. # sqrt transformation {sqrt.x &lt;- sqrt(data$nstorm) h &lt;- hist(sqrt.x, breaks = 10, col = &quot;grey&quot;)} 4.2 Treating outliers Outliers are usually extreme values in the datasets. If not detected and handled appropriately, they can affect the accuracy of the predicitions and analysis. Treating outliers depends on a good knowledge of the data and it is up to the data analyst to decide on how to go about on fixing them. Selva Prabhakaran gave some ways of handling outliers that we’re going to show below. You can detect outliers using boxplot and they will show up as dots outside the whiskers: # For continuous variable outlier_values &lt;- unique(boxplot.stats(data$nstorm)$out) # outlier values. boxplot(data$nstorm, main=&quot;How many storm have you experienced?&quot;, boxwex=0.1) mtext(paste(&quot;Outliers: &quot;, paste(outlier_values, collapse=&quot;, &quot;)), cex=0.6) # For categorical variable boxplot(nstorm ~ worry, data=data, main=&quot;Number of storms experienced across levels of worry&quot;) Approaches for outlier treatment include imputation with mean, median, or mode. We can also cap the values, predict them, or delete the observations. We’ll talk more about that below. 4.3 Fixing missing values Ways of detectng missing values (NA) include the summary and anyNA(). anyNA(data$nstorm) ## [1] TRUE summary(data$nstorm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 2.000 2.000 2.584 3.000 20.000 50 The easiest way to handle them is by removing all of the corresponding observations using tidyr::drop_na(). Or, in some cases, removing the variable itself. tidyr::drop_na(data) ## # A tibble: 776 x 13 ## nstorm worry nevac prepared homeloc nyear gender income politics age zone ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2 3 0 3 2 86 1 4 3 86 A ## 2 3 4 0 3 3 19 2 6 2 83 A ## 3 3 6 1 1 1 52 1 5 3 64 A ## 4 2 1 0 2 3 15 1 5 3 66 A ## 5 1 3 0 3 3 4 1 3 1 38 A ## 6 2 6 0 2 2 17 2 6 2 50 A ## 7 3 5 0 2 3 70 1 4 4 71 A ## 8 2 7 0 2 1 24 2 5 2 60 A ## 9 1 3 1 3 2 2 1 6 4 51 A ## 10 2 3 1 2 1 64 2 5 3 64 A ## # … with 766 more rows, and 2 more variables: lat &lt;dbl&gt;, long &lt;dbl&gt; However, if dropping all of the rows with missing values affect the quality of the data, then another option is to replace the missing values with the mean/median/mode of the variable or predict using an appropriate algorithm. There are several packages out there that are solely dedicated to treating missing values including VIM and MICE. In this next example, we’ll try to predict the 15 missing values in the variable nstorm (number of storms the survey respondents have experienced) using the variables that has no missing values: zone, lat, and long. # Imputation using MICE library(mice) # Building the mice model mice_model &lt;- mice(select(data, zone, lat, long, nstorm), method=&quot;rf&quot;, printFlag=F) ## Warning: Number of logged events: 1 # Predicting the missing values # generate the completed data mice_prediction &lt;- complete(mice_model) # checking for NAs anyNA(mice_prediction) ## [1] FALSE Then we can visualize the data to see how well the imputation has performed. However, the best way to assess the accuracy is to compare actual values with predicted values using measures such as: MSE, MAE, MAPE, etc. # Visualizing the prediction non_na_latitude &lt;- data$lat[!is.na(data$nstorm)] non_na_nstorm &lt;- data$nstorm[!is.na(data$nstorm)] na_latitude &lt;- mice_prediction$lat[is.na(data$nstorm)] na_nstorm &lt;- mice_prediction$nstorm[is.na(data$nstorm)] plot(non_na_nstorm, non_na_latitude, col=&quot;grey&quot;, pch=&quot;•&quot;, ylab=&quot;Latitude&quot;, xlab=&quot;Number of Storms Experienced&quot;) points(na_nstorm, na_latitude, col=&quot;red&quot;, pch=&quot;•&quot;, cex=2) legend(&quot;topright&quot;, c(&quot;Existing values&quot;, &quot;Predicted missing values&quot;), col=c(&quot;grey&quot;, &quot;red&quot;), pch=&quot;•&quot;, cex=1.5) Other ways of imputing the missing values are with mean, median, or mode. prediction &lt;- data na.observations &lt;- is.na(data$nstorm) prediction$nstorm[na.observations] &lt;- median(prediction$nstorm[!na.observations]) # Visualizing the prediction na_latitude &lt;- prediction$lat[na.observations] na_nstorm &lt;- prediction$nstorm[na.observations] plot(non_na_nstorm, non_na_latitude, col=&quot;grey&quot;, pch=&quot;•&quot;, ylab=&quot;Latitude&quot;, xlab=&quot;Number of Storms Experienced&quot;) points(na_nstorm, na_latitude, col=&quot;red&quot;, pch=&quot;•&quot;, cex=2) legend(&quot;topright&quot;, c(&quot;Existing values&quot;, &quot;Predicted missing values&quot;), col=c(&quot;grey&quot;, &quot;red&quot;), pch=&quot;•&quot;, cex=1.5) For mode, there is no built-in function in R but I found one here: getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } "],
["visualization.html", "Section 5 Visualization 5.1 Simple graphs with ggplot2 5.2 Animated graph with gganimate 5.3 Web app with RShiny 5.4 More dataviz resources", " Section 5 Visualization 5.1 Simple graphs with ggplot2 Reading and recoding data library(haven) library(ggplot2) library(tidyr) library(dplyr) raw.data &lt;- read_sav(&#39;data/ypccc-hurricane.sav&#39;) raw.data &lt;- raw.data %&gt;% filter(raw.data$Q1==1 || raw.data$Q2==1) raw.data &lt;- raw.data %&gt;% filter(!is.na(raw.data$Q3), raw.data$Q3!=2, raw.data$Q3!=3) raw.data$S5_cluster &lt;- recode(as.character(raw.data$S5_cluster), &#39;1&#39;=&#39;DieHards&#39;, &#39;2&#39;=&#39;Reluctant&#39;, &#39;4&#39;=&#39;Optimists&#39;, &#39;3&#39;=&#39;Constrained&#39;, &#39;5&#39;=&#39;First Out&#39;) raw.data$S5_cluster &lt;- factor(raw.data$S5_cluster, ordered=T, levels=c(&#39;First Out&#39;, &#39;Constrained&#39;, &#39;Optimists&#39;, &#39;Reluctant&#39;, &#39;DieHards&#39;)) data &lt;- raw.data[!is.na(raw.data$S5_cluster),] 5.1.1 Setting ggplot2 theme plot_theme &lt;- theme( legend.title = element_blank(), legend.box.background = element_rect(), legend.box.margin = margin(6, 6, 6, 6), legend.background = element_blank(), legend.text = element_text(size=12), legend.key.size = unit(1, &quot;cm&quot;) ) + theme( axis.text = element_text(size=12, face=&#39;bold&#39;), axis.line = element_line(size = 1, colour = &quot;#212F3D&quot;), axis.title.x = element_blank(), axis.title.y = element_blank() ) + theme( plot.title = element_text(size = 20, face=&#39;bold&#39;, hjust = 0.5, margin=margin(6,6,6,6)), plot.background=element_blank() ) + theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line(size=1) ) 5.1.2 ggplot2 Example 1: Stacked bar plot # Q31. Is your home located in a hurricane evacuation zone, or not? data$Q31 &lt;- as_factor(data$Q31) data$Q31 &lt;- factor(data$Q31, ordered=T, levels=c(&#39;No&#39;, &#39;Not Sure&#39;, &#39;Yes&#39;)) Q31_data &lt;- data %&gt;% select(Q31, S5_cluster, Final_wgt_pop) %&gt;% # na.omit() %&gt;% group_by(S5_cluster, Q31) %&gt;% summarise(Q31.wgt = sum(Final_wgt_pop)) %&gt;% mutate(freq.wgt = round(100*Q31.wgt/sum(Q31.wgt))) %&gt;% filter(!is.na(Q31)) ggplot(Q31_data, aes(x=S5_cluster, y=freq.wgt, fill=Q31)) + geom_bar(aes(fill=Q31), width=.7, stat=&#39;identity&#39;) + geom_text(aes(label=freq.wgt, vjust=.5), position = position_stack(vjust = 0.5), colour=&#39;white&#39;, size=6, fontface=&#39;bold&#39;) + scale_fill_manual(values=c(&#39;#DC7633&#39;, &#39;#5D6D7E&#39;, &#39;#5499C7&#39;), breaks=c(&#39;Yes&#39;, &#39;Not Sure&#39;, &#39;No&#39;)) + scale_y_continuous(labels = function(x) paste0(x, &quot;%&quot;), expand = c(0, 0), limits = c(0, 100)) + labs(title = &quot;% of CT coastal residents who understand\\nthat their home is in an evacuation zone&quot;) + theme(legend.position=&quot;top&quot;) + plot_theme + theme(legend.box.background = element_blank(), legend.spacing.x = unit(.2, &#39;cm&#39;)) 5.1.3 ggplot2 Example 2: Grouped bar plot # Q23. On a scale of 0%-100%, with 0% being it definitely will NOT happen and 100% being it definitely WILL happen, # how likely do you think it is that each of the following types of hurricane will hit somewhere along the # Connecticut coast in the next 50 years? Q23_data &lt;- data %&gt;% select(S5_cluster, Q23_1, Q23_2, Q23_3, Q23_4, Final_wgt_pop) %&gt;% pivot_longer(-one_of(&#39;S5_cluster&#39;, &#39;Final_wgt_pop&#39;), names_to=&quot;Category&quot;, names_prefix = &quot;Q23_&quot;, values_to = &quot;Q23&quot;) %&gt;% mutate(Category = paste(&quot;Category&quot;, Category), Q23.wgt = Q23 * Final_wgt_pop) %&gt;% na.omit() %&gt;% group_by(S5_cluster, Category) %&gt;% summarise(freq.wgt = round(sum(Q23.wgt, na.rm=T)/sum(Final_wgt_pop, na.rm=T))) ggplot(Q23_data, aes(x=S5_cluster, y=freq.wgt, fill=Category)) + geom_bar(aes(fill=Category), width=.7, position=position_dodge(.8), stat=&#39;identity&#39;) + geom_text(aes(label=freq.wgt, vjust=1.2), position=position_dodge(.8), colour=&#39;white&#39;, size=6, fontface=&#39;bold&#39;) + scale_fill_manual(values=c(&#39;#F6DDCC&#39;, &#39;#EDBB99&#39;, &#39;#E59866&#39;, &#39;#DC7633&#39;)) + scale_y_continuous(labels = function(x) paste0(x, &quot;%&quot;), expand = c(0, 0), limits = c(0, 100)) + labs(title = &quot;Average perception of each segment that\\na Category 1, 2, 3, or 4 hurricane will occur\\nin the next 50 years&quot;) + theme(legend.position=&quot;top&quot;) + plot_theme + theme(legend.box.background = element_blank(), legend.spacing.x = unit(.2, &#39;cm&#39;)) 5.1.4 ggplot2 Example 3: Scatterplot # Scatterplot of state-level average annual temperature by avg ann precipitation (values and anomalies) # CAweather &lt;- read.csv(&#39;CA_Weather.csv&#39;) # missing dataset so go to original source and download manually # Data source: https://www.ncdc.noaa.gov/cag/statewide/time-series # Need to set base period from 1978 to 1998 in Options dialog and set parameters before downloading (12-Month) ca.pcp &lt;- read.csv(&#39;data/4-pcp-12-12-1978-2020.csv&#39;, skip=4) ca.tavg &lt;- read.csv(&#39;data/4-tavg-12-12-1978-2020.csv&#39;, skip=4) names(ca.pcp) &lt;- c(&#39;Year&#39;, &#39;Precip&#39;, &#39;Precip.Anomaly&#39;) names(ca.tavg) &lt;- c(&#39;Year&#39;, &#39;Temp&#39;, &#39;Temp.Anomaly&#39;) ca.weather &lt;- left_join(ca.tavg, ca.pcp, by=&#39;Year&#39;) %&gt;% mutate(Year=as.integer(substr(Year, 1, 4)), Period=if_else(Year&lt;1990, &#39;Before 1990&#39;, if_else(Year&lt;2000, &#39;1990-2000&#39;, if_else(Year&lt;2010, &#39;2000-2010&#39;, &#39;Since 2010&#39;)))) %&gt;% mutate(Period=factor(Period, c(&#39;Before 1990&#39;, &#39;1990-2000&#39;, &#39;2000-2010&#39;, &#39;Since 2010&#39;))) s &lt;- ggplot(data = ca.weather, aes(x=Temp.Anomaly, y=Precip.Anomaly)) hottest.v &lt;- max(ca.weather$Temp.Anomaly) hottest.y &lt;- filter(ca.weather, Temp.Anomaly==hottest.v)$Year driest.v &lt;- min(ca.weather$Precip.Anomaly) driest.y &lt;- filter(ca.weather, Precip.Anomaly==driest.v)$Year s + geom_point(aes(color=Period), size=2) + geom_smooth() + geom_label(label=if_else(ca.weather$Temp.Anomaly==hottest.v, paste(&#39;Hottest Year\\n&#39;, hottest.y), NULL), nudge_x = -.2, nudge_y = 3) + geom_label(label=if_else(ca.weather$Precip.Anomaly==driest.v, paste(&#39;Driest Year\\n&#39;, driest.y), NULL), nudge_x = -.4, nudge_y = 0) + labs(title=&#39;Hotter and Drier Wildfire Seasons in California&#39;, subtitle=&#39;Precipitation vs. temperature anomalies in California from January to October over 40 years (1978-2020)&#39;, caption=&#39;Data from NOAA&#39;, y=&#39;Precipitation anomaly (inches)&#39;, x=&#39;Temperature anomaly (°F)&#39;) + theme_light() # save as png file # ggsave(file = &#39;CA.weather.png&#39;, height=9, width=12) # a simpler one ggplot(ca.weather, aes(Year, Temp.Anomaly)) + geom_line(color=&#39;#0D47A1&#39;) + geom_point(aes(color=Temp.Anomaly)) + geom_hline(yintercept=0, linetype=2, color=&#39;#aaaaaa&#39;) + labs(title=&#39;California Temperature Anomaly 1978-2020&#39;) + scale_color_gradient(low=&#39;blue&#39;, high=&#39;red&#39;) + theme_classic() 5.2 Animated graph with gganimate We can do this by simply adding gganimate code to the previous plot. library(gganimate) animated.plot &lt;- ggplot(ca.weather, aes(Year, Temp.Anomaly)) + geom_line(color=&#39;#0D47A1&#39;) +geom_point(aes(color=Temp.Anomaly)) + geom_hline(yintercept=0, linetype=2, color=&#39;#aaaaaa&#39;) + labs(title=&#39;California Temperature Anomaly 1978-2020&#39;) + scale_color_gradient(low=&#39;blue&#39;, high=&#39;red&#39;) + theme_classic() + # labels geom_label(aes(label=as.character(Year)), nudge_y=.3) + # gganimate code transition_reveal(seq_along(Year)) # animate(animated.plot) ## Interactive graph with plotly Creating an interactive version of the previous graphs is pretty simple using plotly. You just have to separate the data by group. 5.2.1 plotly Example 1: Interactive version of the stacked bar library(plotly) Q31.yes &lt;- Q31_data %&gt;% filter(Q31==&#39;Yes&#39;) Q31.notsure &lt;- Q31_data %&gt;% filter(Q31==&#39;Not Sure&#39;) Q31.no &lt;- Q31_data %&gt;% filter(Q31==&#39;No&#39;) plot_ly(x=Q31.yes$S5_cluster, y=Q31.yes$freq.wgt, type=&#39;bar&#39;, name=&#39;Yes&#39;, marker = list(color = &#39;#DC7633&#39;), text = Q31.yes$freq.wgt, textposition = &#39;auto&#39;) %&gt;% add_trace(x=Q31.notsure$S5_cluster, y=Q31.notsure$freq.wgt, name=&#39;Not Sure&#39;, marker = list(color = &#39;#5D6D7E&#39;)) %&gt;% add_trace(x=Q31.no$S5_cluster, y=Q31.no$freq.wgt, name=&#39;No&#39;, marker = list(color = &#39;#5499C7&#39;)) %&gt;% layout(barmode = &#39;stack&#39;) You can find more ezamples of using plotly on their website: https://plotly.com/r/. 5.3 Web app with RShiny This Shiny tutorial was edited based on the official tutorial on the website With RShiny, it is possible to make the functions in your R script available to people who don’t necessarily know R. For example, in this app, you can create different types of graph by selecting a site and other parameters. Basically, it’s web development using the power of R libraries. So, just like any web tools, an RshinyApp has components on the User and Server side. The visual appearance of the app can be modified in the user component. You can use it to change layout, font size, color, etc. Whereas on the server side, you can customize how your app responds to user inputs/interactions. A basic ShinyApp starter template looks like this: library(shiny) ui &lt;- fluidPage() server &lt;- function(input, output){} shinyApp(ui=ui, server=server) If you run the script above, it will give you and empty page. Some content will show when you add some (text) elements with fluidPage. ui &lt;- fluidPage(&#39;Hello world&#39;) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) 5.3.1 Input functions For richer content, the shiny package has built-in functions that will allow you create them. sliderInput for example adds a slider in your web app. ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100) ) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) sliderInput is part of a group of function called inputs. These functions allow an Rshiny developper like yourself to add html element that will serve as user input. Here are some input functions you can try: All input functions take as first 2 arguments: inputId and label. inputId is an ID that R will use create the input element in a webpage and to reference it later. label is just a text that the user will see, describing what the input element is for. The rest of the arguments are function-specific. 5.3.2 Output functions Output functions, on the other hand, allow you to add outputs of R into your web page. As you know, these outputs can be image, plot, table, text, etc. And there are specific output functions for each type of output. Rshiny output functions Output functions are called similarly to the input functions: Here is our app with an output function: ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), plotOutput(&#39;hist&#39;) ) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) Running the previous script won’t show any plot though. It just reserve a space in the webpage for plot with id: ‘hist’. To actually create the plot, you must use the server function. 5.3.3 Server function Server function creates the interactivity in your web application, i.e. this is where you set up how an output (ex: graph) chnages with the user input (ex: some number). However, there are 3 rules that has to be followed and it is summarized in this script: function(input, output){ #1 outputId is the outputId you defined with output function #2 to render on a webpage. To render a plot, use renderPlot() output$outputId &lt;- renderSomething({ #3 inputId is the inputId you defined with input function someFunction(input$inputId) }) } # Anything inside the curly braces is an R code. And it can be multiple lines of code. In summary, you must define output variable by the outputId and prefixing it with output$. You must call the R script that will produce the output with a render*({}) function. And you must call the user input with its inputId and prefixing it with input$. In our example script, we can render it as follow: ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), plotOutput(&#39;hist&#39;) ) server &lt;- function(input, output){ output$hist &lt;- renderPlot({ title &lt;- paste(input$num, &#39;random normal values&#39;) hist(rnorm(input$num), main=title, xlab=&#39;values&#39;) }) } shinyApp(ui=ui, server=server) To make much more advanced app, simply read the instructions on Rshiny website. 5.3.4 Sharing your app Now you can create your own Shiny app. But for now you can only run it in your computer and no one else has access to it. To make available to the public: Save the ui and server objects/scripts into a stand alone R script, name it app.R, and save it in a separate folder. You must name it app.R as that’s the file that the server will look for when you deploy your app. For this example, I’d save this as app.R ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), plotOutput(&#39;hist&#39;) ) server &lt;- function(input, output){ output$hist &lt;- renderPlot({ title &lt;- paste(input$num, &#39;random normal values&#39;) hist(rnorm(input$num), main=title, xlab=&#39;values&#39;) }) } Go to shinyapps.io and log in or sign for an account. Now simply run the app on your computer In the top right cover of R viewer window, there is a Publish button that you can use to publish your app. Simply follow the instructions. 5.4 More dataviz resources More data visualization resources can be found in this spreadsheet. "],
["spatial.html", "Section 6 Spatial data 6.1 Importing spatial data 6.2 Processing 6.3 Some Visualization Examples", " Section 6 Spatial data In R, spatial data format such as shapefile, geodatabase, geojson, etc. can be read and processed as a dataframe just like common data formats (csv, excel, etc.). However, we have to use special libraries to do so. In addition, because of the nature of this data, spatial relationship operations can also be performed to join or create new features. In this tutorial, we’ll make some examples using the sf and raster libraries. 6.1 Importing spatial data Spatial data have many formats and the most commonly used is shapefile for vectors and tiff for rasters. This tutorial will focus on these 2 types of data. 6.1.1 Reading shapefile The shapefile format consists of at least 3 files: the main file (.shp), the index file (.shx), and the table file (.dbf). Sometimes there is also a projection file (.prj). Please visit this webpage for more details. Because of this file structure, it is good practice to save shapefile in a folder or .zip file. And when reading the data in R, we simply point to the .shp file in the folder. ## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0 ## tibble [27,459 × 11] (S3: sf/tbl_df/tbl/data.frame) ## $ ID : int [1:27459] 1 2 3 4 5 6 7 8 9 10 ... ## $ TSEVENT_ID: int [1:27459] 3 9 10 10 10 10 10 11 27 27 ... ## $ YEAR : int [1:27459] -1610 -479 -426 -426 -426 -426 -426 -373 142 142 ... ## $ MONTH : int [1:27459] NA NA 6 6 6 6 6 NA NA NA ... ## $ DAY : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ DATE_STRIN: chr [1:27459] &quot;-1610/??/??&quot; &quot;-0479/??/??&quot; &quot;-0426/06/??&quot; &quot;-0426/06/??&quot; ... ## $ ARR_DAY : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ ARR_HOUR : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ ARR_MIN : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ TRAV_HOURS: int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ geometry :sfc_POINT of length 27459; first list element: &#39;XY&#39; num [1:2] 2782987 4232038 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ## ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;ID&quot; &quot;TSEVENT_ID&quot; &quot;YEAR&quot; &quot;MONTH&quot; ... When looking at the dataframe structure above, we can see that the shapefile is processed similarly to a regular dataframe, except for the additional geometry field which contain the spatial information. If we want to plot this data, we can simply use the base plot function. # a plot of the first 4 columns # plotted as location plot(tsunami[, 1:4]) # a plot of only columns # plotted as values without geometry plot(tsunami$DEATHS) plot(tsunami$LONGITUDE, tsunami$DIST_FROM_) If you only want to plot the geometry (points): plot(st_geometry(tsunami)) 6.1.2 Reading raster Raster .tif files can be imported with the raster library. library(raster) wind_jan &lt;- raster(&#39;data/wc2.1_10m_wind/wc2.1_10m_wind_01.tif&#39;) plot(wind_jan) 6.2 Processing 6.2.1 Creating point data This example shows how to convert a data with latitude, longitude coordinates to a spatial dataframe. We’ll use the charcoal records from the GCD package as example. library(GCD) data(&quot;paleofiresites&quot;) df.table &lt;- paleofiresites wgs84crs &lt;- 4269 sf.table &lt;- st_as_sf(df.table, coords=c(&#39;long&#39;, &#39;lat&#39;), crs=wgs84crs, remove=F) # non spatial data plot(df.table[,c(7,11:13)]) # spatial data plot(sf.table[,c(7,11:13)]) 6.2.2 Joining with another dataframe Oftentimes, we want to join a non-spatial data with their geographic location. For example, if we have information on energy production and consumption by state, we can join it with spatial feature for better visualization. # get energy data energy.data &lt;- read.csv(&#39;data/energy-by_state.csv&#39;) # use USABoundaries library to get US spatial data library(USAboundaries) us.geo &lt;- us_states(resolution=&#39;low&#39;) joined.data &lt;- us.geo %&gt;% inner_join(energy.data, by=c(&#39;state_abbr&#39;=&#39;State&#39;)) joined.data.albers &lt;- st_transform(joined.data, crs=5070) plot(joined.data.albers[,&#39;Production..U.S..Share&#39;]) 6.2.3 Spatial joining If we look at the paleofiresites data, we can see that we don’t have the US state corresponding to each point location. We can create this variable with spatial join. # get paleo sites for the US us.paleo &lt;- paleofiresites %&gt;% filter(country==&#39;USA&#39;) %&gt;% st_as_sf(coords=c(&#39;long&#39;, &#39;lat&#39;), crs=wgs84crs) # set to the same coordinate system us.paleo.albers &lt;- st_transform(us.paleo, crs=5070) us.geo.albers &lt;- st_transform(us.geo, crs=5070) # spatial join # here we join by within sp.join &lt;- st_join(us.paleo.albers, us.geo.albers, join=st_within) {plot(st_geometry(us.geo.albers)) plot(sp.join[, &#39;state_name&#39;], add=T, pch=16)} For more on what you can do with a vector files in r, please visit sf library reference or read the book Geocomputation with R. 6.3 Some Visualization Examples 6.3.1 Plotting charcoal data # get world map from rnaturalearth library library(rnaturalearth) world &lt;- ne_countries(scale=&#39;small&#39;, returnclass=&#39;sf&#39;) # project to robinson projection sf.world.robin &lt;- st_transform(world, crs=&#39;+proj=robin&#39;) # use imported graticule data for viz grat &lt;- st_geometry(read_sf(&#39;data/ne_110m_graticules_all/ne_110m_wgs84_bounding_box.shp&#39;)) # get paleo sites paleosites &lt;- paleofiresites %&gt;% mutate(site_name = trimws(as.character(site_name))) %&gt;% st_as_sf(coords=c(&#39;long&#39;, &#39;lat&#39;), crs=4326) %&gt;% st_transform(crs=&#39;+proj=robin&#39;) newCoods &lt;- st_coordinates(paleosites) paleosites$Lon &lt;- newCoods[,1] paleosites$Lat &lt;- newCoods[,2] # set ggplot theme library(ggplot2) library(ggrepel) ggTheme &lt;- theme( legend.position=&#39;none&#39;, panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.background = element_blank(), plot.background = element_rect(fill=&#39;#e6e8ed&#39;), panel.border = element_blank(), axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), plot.title = element_text(size=22) ) # few sites to be highlighted selected.sites &lt;- c( &#39;Little Molas Lake&#39;,&#39;Xindi&#39;,&#39;Laguna Oprasa&#39;,&#39;Morro de Itapeva&#39;, &#39;Aracatuba&#39;, &#39;Lago del Greppo&#39;,&#39;Nursery Swamp&#39;, &#39;Huguangyan Maar Lake&#39; ) # mapping map &lt;- ggplot(data=sf.world.robin) + geom_sf(data=grat, fill=&#39;white&#39;, color=&#39;white&#39;) + geom_sf(fill=&#39;#c1cdcd&#39;, size=.1, color=&#39;white&#39;) + geom_point( data=paleosites, aes(x=Lon, y=Lat), color=&#39;#009ACD&#39;,size=1) + geom_point( data=paleosites %&gt;% filter(site_name %in% selected.sites), aes(x=Lon, y=Lat), color=&#39;#FF0000&#39;,size=2) + geom_text_repel( data=paleosites %&gt;% filter(site_name %in% selected.sites), aes(x=Lon, y=Lat, label=site_name), color=&#39;#000000&#39;, size=4) + coord_sf(datum=st_crs(54030)) + ggTheme map "],
["analysis.html", "Section 7 Analysis 7.1 Working with survey data 7.2 Working with COVID-19 data", " Section 7 Analysis 7.1 Working with survey data We’ll use a survey data from a study of approach to engage christians in the issue of global warming. Using this data allow us to compare our results with the original. In this exercise, we’ll replicate the application of sampling weights and the standardization of variables which are common treatments of raw survey data. ## Selected variables ## # A tibble: 13 x 2 ## V1 V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 subjID &quot;Participant ID … ## 2 GWHap &quot;On a scale from 1 to 7, how strongly do you believe that global… ## 3 GWworry &quot;How worried are you about global warming? … ## 4 ex_ppgender &quot;What is your gender? … ## 5 ex_ppage &quot;What is your age? … ## 6 raceCat &quot;Race … ## 7 ex_ppstaten &quot;What U.S. State do you live in? Or do you live in a U.S. Territ… ## 8 moral_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 9 relig_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 10 health_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 11 moral_post &quot;In your opinion, how much do you think environmental protection… ## 12 relig_post &quot;In your opinion, how much do you think environmental protection… ## 13 health_post &quot;In your opinion, how much do you think environmental protection… 7.1.1 Variable Standardization Standardizing variables must be performed before any regression or other statistical analysis are done with the data. It can be done by rescaling the variables using the z-score formula. Standardized variables will then have a mean of 0 and a standard deviation of 1. # z-score formula get.z.score &lt;- function(X){ (X-mean(X, na.rm=T))/sd(X, na.rm=T) } # standardizing the pretest and posttest variables Zdata &lt;- data %&gt;% mutate(across(ends_with(&#39;_pre&#39;)|ends_with(&#39;_post&#39;), get.z.score, .names=&#39;Z{col}&#39;)) Zdata ## # A tibble: 1,620 x 19 ## subjID GWHap GWworry ex_ppgender ex_ppage raceCat ex_ppstaten moral_pre ## &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; ## 1 101 7 [I s… 6 [6] 2 [Female] 69 2 [Bla… 36 [Ohio] 4 [4] ## 2 102 4 [I a… 3 [3] 1 [Male] 30 1 [Whi… 41 [South … 4 [4] ## 3 103 6 [6] 7 [I a… 2 [Female] 31 1 [Whi… 36 [Ohio] 5 [5] ## 4 104 7 [I s… 6 [6] 2 [Female] 41 1 [Whi… 43 [Tennes… 7 [Very … ## 5 105 2 [2] 2 [2] 2 [Female] 28 1 [Whi… 14 [Illino… 4 [4] ## 6 106 5 [5] 4 [4] 2 [Female] 52 1 [Whi… 33 [New Yo… 4 [4] ## 7 107 5 [5] 6 [6] 1 [Male] 36 4 [His… 36 [Ohio] 7 [Very … ## 8 108 7 [I s… 6 [6] 2 [Female] 19 3 [Oth… 33 [New Yo… 6 [6] ## 9 109 6 [6] 5 [5] 1 [Male] 21 2 [Bla… 34 [North … 5 [5] ## 10 110 7 [I s… 5 [5] 1 [Male] 26 1 [Whi… 25 [Missis… 6 [6] ## # … with 1,610 more rows, and 11 more variables: relig_pre &lt;dbl+lbl&gt;, ## # health_pre &lt;dbl+lbl&gt;, moral_post &lt;dbl+lbl&gt;, relig_post &lt;dbl+lbl&gt;, ## # health_post &lt;dbl+lbl&gt;, Zmoral_pre &lt;dbl&gt;, Zrelig_pre &lt;dbl&gt;, ## # Zhealth_pre &lt;dbl&gt;, Zmoral_post &lt;dbl&gt;, Zrelig_post &lt;dbl&gt;, Zhealth_post &lt;dbl&gt; 7.1.2 Sample weighting Per Josep Espaga Reig’s book, there are 4 steps in survey weighting: Base/design weights: if participants have different probability of being sampled; Non-response weights: if some participants didn’t respond to the survey; Use of auxillary data/calibration: adjusting weights to the total population; Analysis of weight variability/trimming: to check variability in the computed weights; In this example, we will estimate of the percent of US christian population who either believe global warming (GW) is happening or worried about GW. We will give descriptives of the distribution of these two variables and then a simple extrapolation and compute total for the whole US christian population. Our variables of interest are: GWHap: On a scale from 1 to 7, how strongly do you believe that global warming is or is not happening? GWworry: How worried are you about global warming? 7.1.2.1 Step 1: Design weights In the paper, it is mentionned that participants were recruited via Prime Panels. And only those who previously identified as christians were selected. It should be good to assume that each participants had an equal probability of being sampled. Therefore, we can skip the Step 1 of the weighting. If we had to compute the design weight \\(d_i\\), it would be equal to the inverse of the probability of being sampled: \\(1/p_i\\). Per Josep Espaga Reig’s book: the design weights can be interepreted as “the number of units in our population that each unit in our sample represents. […] The sum of all design weights should be equal to the total number of units in our population”. 7.1.2.2 Step 2: Non-response weights The paper also doesn’t mention any information about non-respondent during the survey. So we can assume that there is no bias in the responses and we can skip Step 2. In short, the goal of Step 2 is to account for differences in propensity to respond among the participants. For example, participants from specific neighborhood, income level, etc may have lower response rates, and all the response we have are from a specific demographic groups, which would bias the analysis. 7.1.2.3 Step 3: Using auxillary data for weight calibration The goal here is to calibrate the survey data to the population in general. So we would need information on both the survey respondents and the population. In this case, we will use demographic variables (gender, education, and race). One of the most common method for calibration weighting is called: raking which we will use as an the example. We’ll use as auxiliary data the PEW Datasets which gives some statistics of the US christian population. We’ll scrape the data directly from their website so that we’ll have fewer data wrangling to do. # PEW data # getting only the christian respondent and their weights pew.data &lt;- read_sav(&#39;data/pew-data.sav&#39;) %&gt;% select(RELTRAD, agerec, SEX, racethn, WEIGHT) %&gt;% mutate(across(where(is.labelled), as_factor)) %&gt;% mutate(across(where(is.factor), as.character)) %&gt;% mutate( religion = RELTRAD %&gt;% recode( `Evangelical Protestant Tradition`=&#39;christian&#39;, `Mainline Protestant Tradition`=&#39;christian&#39;, `Historically Black Protestant Tradition`=&#39;christian&#39;, `Catholic`=&#39;christian&#39;, `Mormon`=&#39;christian&#39;, `Orthodox Christian`=&#39;christian&#39;, `Jehovah&#39;s Witness`=&#39;christian&#39;, `Other Christian`=&#39;christian&#39;, `Jewish`=&#39;other&#39;, `Muslim`=&#39;other&#39;, `Buddhist`=&#39;other&#39;, `Hindu`=&#39;other&#39;, `Other World Religions`=&#39;other&#39;, `Other Faiths`=&#39;other&#39;, `Unaffiliated (religious &quot;nones&quot;)`=&#39;other&#39;, `Don&#39;t know/refused - no information on religious identity`=&#39;other&#39;, ), pew.age = agerec %&gt;% recode( `Age 24 or younger`=&#39;18-29&#39;, `Age 25-29`=&#39;18-29&#39;, `30-34`=&#39;30-49&#39;, `35-39`=&#39;30-49&#39;, `40-44`=&#39;30-49&#39;, `45-49`=&#39;30-49&#39;, `50-54`=&#39;50-64&#39;, `55-59`=&#39;50-64&#39;, `60-64`=&#39;50-64&#39;, `65-69`=&#39;65+&#39;, `70-74`=&#39;65+&#39;, `75-79`=&#39;65+&#39;, `80-84`=&#39;65+&#39;, `85-89`=&#39;65+&#39;, `Age 90 or older`=&#39;65+&#39;, `Don&#39;t know/refused`=&#39;NA&#39; ), pew.race = racethn %&gt;% recode( `White non-Hispanic`=&#39;White&#39;, `Black non-Hispanic`=&#39;Black&#39;, `Hispanic`=&#39;Hispanic&#39;, `Other`=&#39;Other&#39;, `Don’t know/Refused (VOL.)`=&#39;NA&#39; ) ) %&gt;% filter(religion==&#39;christian&#39;) %&gt;% select(SEX, pew.age, pew.race, WEIGHT) %&gt;% na.omit() names(pew.data)[1] &lt;- &#39;pew.gender&#39; head(pew.data,10) ## # A tibble: 10 x 4 ## pew.gender pew.age pew.race WEIGHT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Female 65+ White 0.438 ## 2 Female 65+ White 0.384 ## 3 Female 50-64 White 0.663 ## 4 Female 30-49 Hispanic 1.64 ## 5 Female 50-64 White 0.357 ## 6 Female 65+ Black 0.664 ## 7 Female 65+ White 1.42 ## 8 Male 30-49 White 0.388 ## 9 Male 65+ Other 0.238 ## 10 Female 30-49 White 0.882 pew.gender_age &lt;- pew.data %&gt;% select(pew.age, pew.gender, WEIGHT) %&gt;% group_by(pew.gender, pew.age) %&gt;% summarise(n=sum(WEIGHT)) %&gt;% tidyr::pivot_wider(names_from=pew.age, values_from=n) %&gt;% select(-`NA`) pew.race &lt;- pew.data %&gt;% select(pew.race, WEIGHT) %&gt;% group_by(pew.race) %&gt;% summarise(n=sum(WEIGHT)) %&gt;% filter(pew.race!=&#39;NA&#39;) Now we’ll recode our age and race variables to match the PEW data. # interaction between gender and age our_data.recoded &lt;- data %&gt;% select(ex_ppage, ex_ppgender, raceCat) %&gt;% mutate(across(where(is.labelled), as_factor)) %&gt;% mutate( age_group=cut(ex_ppage %&gt;% as.numeric(), breaks=c(18, 30, 50, 65, 100), right=F) ) %&gt;% mutate(across(where(is.factor), as.character)) %&gt;% mutate( age_group=age_group %&gt;% recode( `[18,30)`=&#39;18-29&#39;, `[30,50)`=&#39;30-49&#39;, `[50,65)`=&#39;50-64&#39;, `[65,100)`=&#39;65+&#39; ), race_cat=raceCat %&gt;% recode( `White Non-hispanic`=&#39;White&#39;, `Black Non-hispanic`=&#39;Black&#39;, `Hispanic`=&#39;Hispanic&#39;, `Other Non-hispanic`=&#39;Other&#39;, `Two+ races non-hispanic`=&#39;Other&#39; ) ) %&gt;% tidyr::unite(col=gender_age, ex_ppgender, age_group, remove=F) %&gt;% mutate(gender_age=replace(x=gender_age, list=gender_age %in% c(&#39;Female_NA&#39;, &#39;Male_NA&#39;), values=NA)) %&gt;% na.omit() # number of participants/weights by units # this would be the sum of the final weights from Step 1 and 2 if they were necessary our_data.gender_age &lt;- our_data.recoded %&gt;% group_by(age_group, ex_ppgender) %&gt;% summarise(n = n()) %&gt;% tidyr::pivot_wider(names_from=age_group, values_from=n) our_data.race &lt;- our_data.recoded %&gt;% group_by(race_cat) %&gt;% summarise(n = n()) rm(list=setdiff(ls(), c(&#39;data&#39;, &#39;data.variables&#39;, &#39;Zdata&#39;, &#39;pew.data&#39;, &#39;pew.gender_age&#39;, &#39;pew.race&#39;, &#39;our_data.recoded&#39;, &#39;our_data.gender_age&#39;, &#39;our_data.race&#39;))) Now, we’ll scale the PEW data to our sample size. # our total (weighted) observations our_data.wgt &lt;- nrow(our_data.recoded) # scaling PEW to our data # for age and gender pew.gender_age.scaled &lt;- pew.gender_age %&gt;% tidyr::pivot_longer(-pew.gender, names_to=&#39;pew.age&#39;, values_to=&#39;WEIGHT&#39;) %&gt;% tidyr::unite(col=gender_age, pew.gender, pew.age) total.pew &lt;- pew.gender_age.scaled$WEIGHT %&gt;% sum() pew.gender_age.scaled %&lt;&gt;% mutate(Freq = round(WEIGHT/total.pew * our_data.wgt, 0) ) %&gt;% select(-WEIGHT) pew.gender_age.scaled ## # A tibble: 8 x 2 ## gender_age Freq ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female_18-29 149 ## 2 Female_30-49 289 ## 3 Female_50-64 250 ## 4 Female_65+ 197 ## 5 Male_18-29 131 ## 6 Male_30-49 245 ## 7 Male_50-64 218 ## 8 Male_65+ 141 # for race pew.race.scaled &lt;- pew.race %&gt;% mutate( Freq = round(n/total.pew * our_data.wgt, 0), race_cat=pew.race ) %&gt;% select(-n, -pew.race) pew.race.scaled ## # A tibble: 4 x 2 ## Freq race_cat ## &lt;dbl&gt; &lt;chr&gt; ## 1 209 Black ## 2 262 Hispanic ## 3 85 Other ## 4 1071 White Implementing calibration We’ll use the same procedure explained in this book which uses the survey package. # No weights or probabilities supplied, assuming equal probability our.svydesign &lt;- svydesign(ids = ~ 0, data=our_data.recoded) # the variable used for calibration are: gender/age interaction and race our.raked &lt;- rake(our.svydesign, sample.margins = list(~race_cat, ~gender_age), population = list(pew.race.scaled, pew.gender_age.scaled)) # collecting the weights raked.weight &lt;- our.raked$postStrata[[1]][[1]] %&gt;% attributes() %&gt;% .[[&quot;weights&quot;]] weighted_data &lt;- our_data.recoded weighted_data$raked.weight &lt;- raked.weight head(weighted_data, 10) ## # A tibble: 10 x 7 ## ex_ppage gender_age ex_ppgender raceCat age_group race_cat raked.weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 69 Female_65+ Female Black Non-h… 65+ Black 2.11 ## 2 30 Male_30-49 Male White Non-h… 30-49 White 0.959 ## 3 31 Female_30-… Female White Non-h… 30-49 White 0.688 ## 4 41 Female_30-… Female White Non-h… 30-49 White 0.688 ## 5 28 Female_18-… Female White Non-h… 18-29 White 0.361 ## 6 52 Female_50-… Female White Non-h… 50-64 White 0.875 ## 7 36 Male_30-49 Male Hispanic 30-49 Hispanic 2.41 ## 8 19 Female_18-… Female Other Non-h… 18-29 Other 0.531 ## 9 21 Male_18-29 Male Black Non-h… 18-29 Black 1.29 ## 10 26 Male_18-29 Male White Non-h… 18-29 White 0.945 7.2 Working with COVID-19 data In this example, we’ll use daily updated data from the JHU Covid Resource Center to create animated time series of coronavirus cases and deaths in the US. The data is available in their GitHub. We’ll use the daily report for the US from 04-12-2020 to 06-22-2020. repo.url &lt;- &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/&#39; # importing all csv us.covid.files &lt;- list() date.format &lt;- &#39;%m-%d-%Y&#39; dates &lt;- format(seq(as.Date(&#39;2020/04/12&#39;), as.Date(as.Date(&#39;2020/06/22&#39;)), by=&#39;day&#39;), date.format) i &lt;- 1 for (date in dates){ us.covid.files[[i]] &lt;- read.csv(url(paste0(repo.url, date, &#39;.csv&#39;))) %&gt;% select(Province_State, Confirmed, Deaths, Recovered, Mortality_Rate) %&gt;% mutate(Date=as.Date(date, date.format), Confirmed.rank=rank(-Confirmed), Deaths.rank=rank(-Deaths), Mortality.rank=rank(-Mortality_Rate)) i &lt;- i+1 } head(us.covid.files[[1]]) ## Province_State Confirmed Deaths Recovered Mortality_Rate Date ## 1 Alabama 3563 93 NA 2.610160 2020-04-12 ## 2 Alaska 272 8 66 2.941176 2020-04-12 ## 3 Arizona 3542 115 NA 3.246753 2020-04-12 ## 4 Arkansas 1280 27 367 2.109375 2020-04-12 ## 5 California 22795 640 NA 2.812020 2020-04-12 ## 6 Colorado 7307 289 NA 3.955112 2020-04-12 ## Confirmed.rank Deaths.rank Mortality.rank ## 1 22 27 33 ## 2 51 47 27 ## 3 23 21 24 ## 4 39 39 43 ## 5 6 7 29 ## 6 16 14 14 us.covid.df &lt;- do.call(rbind, us.covid.files) # line series by state last.df &lt;- us.covid.files[[length(us.covid.files)]] us.covid.plot &lt;- ggplot(us.covid.df %&gt;% filter(Confirmed.rank &lt;=10), aes(y=Confirmed.rank, group=Province_State, fill=Province_State)) + geom_tile(aes(x=Confirmed/2, width=Confirmed, height=0.9), alpha=0.8, color=NA) + geom_text(aes(x=0, label=paste(Province_State, &#39; &#39;)), hjust=1, vjust=1) + geom_text(aes(x=Confirmed, label=paste0(&#39; &#39;, round(Confirmed/1000), &#39;k&#39;)), hjust=0) + scale_y_reverse() + coord_cartesian(clip=&#39;off&#39;) + theme_bw() + # custom ggtheme theme(legend.position=&#39;none&#39;, axis.line=element_blank(), axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), panel.grid.major.y=element_blank(), panel.grid.minor.y=element_blank(), panel.border=element_blank(), plot.margin = margin(2,1,2,3,&#39;cm&#39;)) us.covid.anim &lt;- us.covid.plot + transition_states(Date, transition_length=4, state_length=1) + view_follow(fixed_x=T) + labs(title = &#39;Daily coronavirus cases : {closest_state}&#39;, subtitle = &#39;Top 10 States&#39;, caption =&#39;Data Source: Johns Hopkins Coronavirus Resource Center&#39;) # animate(us.covid.anim, nframes=400, fps=10, renderer=gifski_renderer(&#39;images/us-covid-rank.gif&#39;)) us-covid-rank The previous animated plot shows the the evolution of the number of cases. And changing to other variables such as Mortality_Rate or Deaths would follow the same steps. Now, if instead we want to show how the evolution of case change over space, we can do mapping instead. library(sf) # first we&#39;ll join with a geometry feature of the US us.states.geo &lt;- read_sf(&#39;data/us-states/us-states.shp&#39;) # adding geometry us.covid.sf &lt;- us.states.geo %&gt;% right_join(us.covid.df, by=c(&#39;state_name&#39;=&#39;Province_State&#39;)) # mapping max_val &lt;- max(us.covid.df$Mortality_Rate) mid_val &lt;- mean(us.covid.df$Mortality_Rate) map &lt;- ggplot(us.covid.sf) + geom_sf(aes(fill=Mortality_Rate), size=.1) + scale_fill_gradient2(high=&#39;#6E2C00&#39;, mid=&#39;#FDFEFE&#39;, low=&#39;#28B463&#39;, midpoint=mid_val, limits = c(0,max_val)) + coord_sf(crs=5070) + theme_bw() + # custom ggtheme theme(legend.position=&#39;bottom&#39;, axis.line=element_blank(), axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), panel.grid.major.y=element_blank(), panel.grid.minor.y=element_blank(), panel.border=element_blank(), plot.margin = margin(1,1,1,1,&#39;cm&#39;)) map.anim &lt;- map + transition_states(Date, transition_length=4, state_length=1) + labs(title = &#39;Daily coronavirus cases : {closest_state}&#39;, caption =&#39;Data Source: Johns Hopkins Coronavirus Resource Center&#39;) # animate(map.anim, nframes=100, fps=5, renderer=gifski_renderer(&#39;images/us-covid-map.gif&#39;)) us-covid-map "],
["resources.html", "Section 8 Resources 8.1 Data 8.2 EDS Course Map", " Section 8 Resources 8.1 Data Here is a non-exhaustive list of data resources for environmental studies. https://archive.ics.uci.edu/ml/datasets/Forest+Fires Region URL Topics World Cities http://www.worldclimate.com/ Temperature, Rainfall, Sea-level US https://climatecommunication.yale.edu/ Yale Program on Climate Change Communication US https://data.census.gov/cedsci/ Census surveys US https://www.epa.gov/ejscreen/download-ejscreen-data Environmental justice US https://www.data.gov/ Agriculture, Climate, Consumer, Ecosystems, Education, Energy, Finance, Health, Local Government, Manufacturing, Maritime, Ocean, Public Safety, Science &amp; Research US https://simplyanalytics.com/ Demography, Business, Health, Marketing US https://www.ncdc.noaa.gov/cdo-web/datatools Climate, Weather US https://www.epa.gov/outdoor-air-quality-data Air quality US https://www.americancommunities.org/methodology/ Communities US https://tidesandcurrents.noaa.gov/products.html Sea water levels US https://www.waterqualitydata.us/ Water quality US https://waterdata.usgs.gov/nwis/qw Water quality US https://hifld-geoplatform.opendata.arcgis.com/ Agriculture, Borders, Boundaries, Chemicals, Commercial, Communications, Education, Emergency Services, Energy, Finance, Food Industry, Geonames, Government, Law Enforcement, Mail Shipping, Mining, National Flood Hazard, Natural Hazards, Public Health, Public Venues, Transportation Air, Transportation Ground, Transportation Water, Water Supply OECD https://data.oecd.org/ Agriculture, Development, Economy, Education, Energy, Environment, Finance, Government, Health, Innovation and Technology, Jobs, Society Multpile https://www.kdnuggets.com/datasets/index.html Multiple Multiple https://www.kaggle.com/datasets Multiple Multiple https://data.fivethirtyeight.com/ Multiple Global https://csssi.yale.edu/ Yale CSSSI Global https://yceo.yale.edu/image-sources Yale Center for Earth Observation Global https://osf.io/ Multiple Global https://data.worldbank.org/ Development Global https://data.humdata.org/ Humanitarian Global https://climatedataguide.ucar.edu/climate-data Geospatial data resources Global http://chelsa-climate.org/downloads/ Climate predictions, timeseries Global https://ourworldindata.org/ Health, Demographic Change, Food and Agriculture, Education and Knowledge, Energy and Environment, Innovation and Technological Change, Poverty and Economic Development, Living conditions, Community and Wellbeing, Human rights and Democracy, Violence and War Global http://aqicn.org/data-platform/register/ Air quality Global https://globalfishingwatch.org/datasets-and-code/ Fishing Global https://www.gbif.org/ Biodiversity occurences Global https://data.globalforestwatch.org/ Forest Change, Land Use, Conservation, People, Forest Cover Global https://resourcewatch.org/data/explore Food, Climate, Energy, Water, Forests, Ocean, Society, Cities Europe https://www.eea.europa.eu/data-and-maps Air pollution, Biodiversity – Ecosystems, Chemicals, Climate change adaptation, Climate change mitigation, Energy, Environment and health, Industry, Land use, Marine, Policy instruments, Resource efficiency and waste, Soil, Specific regions, Sustainability transitions, Transport, Water and marine environment Coastal https://data.unep-wcmc.org/ Ecosystem, Biodiversity 8.2 EDS Course Map Here are the suggested courses at Yale and tracks to improve your data science skills. EDS Course Map "],
["references.html", "References", " References "]
]
