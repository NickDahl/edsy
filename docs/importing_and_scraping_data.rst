.. _importing_and_scraping_data:

=============================
Importing and Scraping data
=============================

.. contents::
   :local:
   :depth: 2


CSV Files
==========

Reading csv with column headers and separated by :code:`,`. These parameters are also the default values for the :code:`read.csv` function.


.. code-block:: r

   data <- read.csv(file = '/path/to/csv', header = TRUE, sep = ',')

   # Example
   data <- read.csv(file = 'eds.data.hurricane.csv', header = TRUE)
   head(data)


.. code-block:: rout

    nstorm worry nevac prepared homeloc nyear gender income politics age zone      lat      long
   1      2     3     0        3       2    86      1      4        3  86    A 41.26324 -72.84297
   2      1     4     0        3       3    25      1     NA       NA  52    A 41.31061 -72.09882
   3      3     4     0        3       3    19      2      6        2  83    A 41.26523 -72.82523
   4      3     6     1        1       1    52      1      5        3  64    A 41.15419 -73.11342
   5      2     1     0        2       3    15      1      5        3  66    A 41.26944 -72.89225
   6      5     4     0        2       1    23      2     NA        3  76    A 41.27140 -72.59094


Excel Files
===========

The main advantage of Excel files is that they can store multiple tables. But reading these tables at once is different from a CSV. For this example, we're going to use the :code:`readxl` package from the :code:`tidyverse` collection. Please visit this `website <https://www.tidyverse.org/>`_ to learn more about :code:`tidyverse`.

To read an excel file, you can use the :code:`read_excel` function and specify at least the :code:`path/to/the/file` and :code:`sheet` you want to open. If you don't specify the :code:`sheet`, :code:`read_excel` will automatically open the first table in the spreadsheet.

.. code-block:: r

   # In the 'eds.excel.sample.xlsx' file, there are 2 tables: heatwave and hurricane
   # Here's how we read both tables into r

   # Loading the library
   library(readxl)

   # Reading the tables
   heatwave <-  read_excel(path='eds.excel.sample.xlsx', sheet = 'heatwave')
   hurricane <-  read_excel(path='eds.excel.sample.xlsx', sheet = 'hurricane')

   # Once the tables are stored into individual r variable, you can perform exploration and analysis with them.


Google Spreadsheets
====================

If the data is stored in a Google spreadsheet, we can read it using the :code:`googledrive` and :code:`googlesheet4` packages from the :code:`tidyverse` collection. We use the :code:`googledrive` package to log into our Google Drive account and :code:`googlesheets4` to read the speadsheets in our drive.

In the example below, I used a spreadsheet named :code:`eds.sample.googlesheets` which contains the same tables in the previous Excel example (heatwave and hurricane). You can clone the spreadsheet via this `link <https://drive.google.com/open?id=1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc>`_ if you'd like to repeat the steps below using your Google account.


.. code-block:: r

   # Logging into Google Drive
   # Loading the library
   library(googledrive)

   # To authenticate and authorize googledrive package
   # When prompted: log in, authorized googledrive, and use the authorization code provided
   drive_auth()
   # Then, to view the list of files in a folder
   drive_ls("EDS") # where "EDS" is the folder name
   # To also get the files within the subfolders
   drive_ls("EDS", recursive = TRUE)
   # To view the list of spreadsheets within a folder
   drive_ls("EDS", type="spreadsheet")

The last line will output the following where you can have the name and id of the Google Sheet you want to open in R:


.. code-block:: rout

   # A tibble: 1 x 3
    name                    id                                           drive_resource   
   * <chr>                   <chr>                                        <list>           
   1 eds.sample.googlesheets 1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc <named list [35]>

Because of Google authentification system, you may run into an error like below when re-running the previous code (using :code:`drive_ls()`).


.. code-block:: rout

   Error in add_id_path(nodes, root_id = root_id, leaf = leaf) : !anyDuplicated(nodes$id) is not TRUE


To avoid this, you can use the folder url instead of the folder name. The folder url can be obtained by right-clicking on the folder and click :code:`Get shareable link`. Then run the following code:


.. code-block:: r

   # If using folder name doesn't work
   folder_url = 'https://drive.google.com/open?id=1e0uJ9dwFcL34JA61F0tGSoaiMZ_xio_4'
   drive_ls(folder_url, type="spreadsheet")


Then you can load the spreadsheet by using its :code:`id`


.. code-block:: r

   eds.sample.spreadsheet <- drive_get(id = '1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc')


It also possible to read the spreadsheet right way by using its link / :code:`path` (without using :code:`drive_ls()`)


.. code-block:: r

   eds.sample.spreadsheet <- drive_get(path = 'https://drive.google.com/open?id=1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc')


Once the spreadsheet is loaded, we run a similar code used for the Excel files to read tables within the spreadsheet. But for Google Sheets, function is called :code:`read_sheet`


.. code-block:: r

   # Loading the library
   library(googlesheets4)
   # Authorizing the googlesheets4 package
   sheets_auth(token=drive_token())
   # Readind the tables
   heatwave <- read_sheet(eds.sample.spreadsheet, sheet = 'heatwave')
   hurricane <- read_sheet(eds.sample.spreadsheet, sheet = 'hurricane')


Web Scraping
=============

Web scraping is the process of fteching a webpage and extracting information / data from it. It is very useful if you want to create a dynamic database that updates based on the content of a specific website.

To scrap a webpage, we first need to know how to get to the webpage, a url that you can use to directly access the content. For example, to obtain the Google search results for "data science", you can simply copy and paste this url to your browser: https://www.google.com/search?q=data+science, without having to type "data science" on Google search web page. Some website like Twitter or Facebook will require to you to use an API and authenticate in order to access some of their data. 

For this example, we're going to use The Weather Channel website which do not require autentification. We'll to extract the 10-day forecast for a specific location and store the data in a dataframe.

After inspecting the website and it's url, I have noticed that you can view the weather data by zip code using this url pattern:

:code:`https://weather.com/weather/` + :code:`forecast type` + :code:`/l/` + :code:`zip_code` + :code:`:4:US`

For example, if we want to view the 10-day forecast for New Haven, we can go to: https://weather.com/weather/tenday/l/06511:4:US. And for today's forecast: https://weather.com/weather/today/l/06511:4:US

Once we have the webpage url, we can read it into R and extract the data using :code:`rvest` from the :code:`tidyverse` collection.

The New Haven 10-forecast webpage looks like this:

.. image:: https://raw.githubusercontent.com/rajaoberison/edsy/master/images/weatherpage.png
   :height: 812px
   :align: center
   :alt: weatherpage

Basically, what we want is the table that have the weather information. In order to extract the values that we want, we have to know where in the source code they are located. For example, in the "DAY" column, we want to extract the `exact date` instead of the `days of the week`. And we can do that by:

* inspecting the tag or class of exact date from the website. Move the cursor to the exact date, right-click, then choose :code:`Inspect`
* then, a window will open, which will point directly to location of the `exact date` in the source code. Take notes of the css (tag or class name), and use it to get the `exact date` value using the :code:`html_nodes()` function.

.. image:: https://raw.githubusercontent.com/rajaoberison/edsy/master/images/webcss.png
   :height: 320px
   :align: center
   :alt: webcss

Here is how we extract the dates:


.. code-block:: r

   # Loading library
   library(rvest)

   # Get the webpage url
   url = 'https://weather.com/weather/tenday/l/06511:4:US'
   # Load the webpage using the url
   webpage <- read_html(url)

   # Getting the exact date
   # Filtering the relevant css / location
   date_locations <- html_nodes(webpage, "span.day-detail.clearfix")
   # Extracting the exact value
   raw_date <- html_text(date_locations)
   # Because the value are formatted like "Nov 21" we have to convert to a date format
   exact_date <- as.Date(raw_date, format="%b %d") # b = month, d = day


.. code-block:: rout

   # raw date
   [1] "NOV 19" "NOV 20" "NOV 21" "NOV 22" "NOV 23" "NOV 24" "NOV 25" "NOV 26" "NOV 27" "NOV 28"
   [11] "NOV 29" "NOV 30" "DEC 1"  "DEC 2"  "DEC 3" 

   # exact_date
   [1] "2019-11-19" "2019-11-20" "2019-11-21" "2019-11-22" "2019-11-23" "2019-11-24" "2019-11-25"
   [8] "2019-11-26" "2019-11-27" "2019-11-28" "2019-11-29" "2019-11-30" "2019-12-01" "2019-12-02"
   [15] "2019-12-03"


And here is the full code that extract the complete table:

.. code-block:: r

   # Loading library
   library(rvest)

   # Get the webpage url
   url = 'https://weather.com/weather/tenday/l/06511:4:US'
   # Load the webpage using the url
   webpage <- read_html(url)

   # Getting the exact date
   # Filtering the relevant css / location
   date_locations <- html_nodes(webpage, "span.day-detail.clearfix")
   # Extracting the exact value
   raw_date <- html_text(date_locations)
   # Because the value are formatted like "Nov 21" we have to convert to a date format
   exact_date <- as.Date(raw_date, format="%b %d") # b = month, d = day

   # Getting the weather description
   desc_loc <- html_nodes(webpage, "td.description")
   desc <- html_text(desc_loc)

   # Getting the temperature
   temp_loc <- html_nodes(webpage, "td.temp")
   temp <- html_text(temp_loc)
   # High and Low temperature values
   high_temp <- rep(NA, length(temp))
   low_temp <- rep(NA, length(temp))
   for (i in 1:length(temp)){
     all <- unlist(strsplit(temp[i], "°"))
     if (length(all) > 1){
       high_temp[i] <- all[1]
       low_temp[i] <- all[2]
     } else {
       low_temp[i] <- 38
     }
   }

   # Getting the precipitation
   precip_loc <- html_nodes(webpage, "td.precip")
   precip <- as.numeric(sub("%", "", html_text(precip_loc))) / 100

   # Getting the wind
   wind_loc <- html_nodes(webpage, "td.wind")
   wind <- html_text(wind_loc)
   # Wind direction and speed
   wind_dir <- rep(NA, length(wind))
   wind_speed <- rep(NA, length(wind))
   for (i in 1:length(wind)){
     all <- unlist(strsplit(wind[i], " "))
     wind_dir[i] <- all[1]
     wind_speed[i] <- all[2]
   }

   # Getting the humidity
   humidity_loc <- html_nodes(webpage, "td.humidity")
   humidity <- as.numeric(sub("%", "", html_text(humidity_loc))) / 100

   # Save the data in tibble
   library(tibble)
   new_haven_forecast <- tibble('day' = exact_date, 'description' = desc,
                                'high_temp' = high_temp, 'low_temp' = low_temp,
                                'precip' = precip, 'wind_dir' = wind_dir,
                                'wind_speed' = wind_speed, 'himidity' = humidity)
                             
                             
.. code-block:: rout

   # A tibble: 15 x 8
      day        description       high_temp low_temp precip wind_dir wind_speed himidity
      <date>     <chr>             <chr>     <chr>     <dbl> <chr>    <chr>         <dbl>
    1 2019-11-19 Cloudy            NA        38          0.1 N        5             0.78 
    2 2019-11-20 Cloudy            44        35          0.1 NNW      11            0.7  
    3 2019-11-21 Mostly Sunny      52        41          0   NW       9             0.55 
    4 2019-11-22 PM Showers        57        31          0.4 WSW      14            0.68 
    5 2019-11-23 Partly Cloudy     46        35          0.1 W        8             0.5  
    6 2019-11-24 AM Showers        44        32          0.4 NNW      11            0.69 
    7 2019-11-25 Mostly Sunny      49        33          0.1 WNW      9             0.6  
    8 2019-11-26 Mostly Sunny      53        41          0.1 W        6             0.62 
    9 2019-11-27 Showers           56        41          0.5 SSE      12            0.76 
   10 2019-11-28 AM Showers        49        36          0.4 WNW      13            0.59 
   11 2019-11-29 PM Showers        47        37          0.4 WNW      11            0.580
   12 2019-11-30 Partly Cloudy     48        40          0.2 NW       10            0.65 
   13 2019-12-01 Showers           46        34          0.4 NW       13            0.59 
   14 2019-12-02 Rain/Snow Showers 44        33          0.3 WNW      13            0.55 
   15 2019-12-03 Partly Cloudy     43        33          0.2 WNW      12            0.570

